{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "from onmt.modules import Embeddings\n",
    "from onmt.encoders import RNNEncoder, TransformerEncoder\n",
    "from onmt.decoders.decoder import StdRNNDecoder\n",
    "from onmt.decoders.transformer import TransformerDecoder\n",
    "\n",
    "from utils import Corpus, batchify, truncate, SOS_IDX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. Adversarial training할 때 discriminator accuracy에 따라 (O)\n",
    "2. Validation step (O)\n",
    "3. Transformer 테스트\n",
    "4. 실제 데이터 Transfer 시켜서 눈으로 확인\n",
    "5. Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 0\n",
    "device = torch.device(\"cuda:{}\".format(gpu_id) if gpu_id != -1 else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "vocab_size = 10000\n",
    "embed_dim = 100\n",
    "\n",
    "bidirectional = True\n",
    "num_layers = 1\n",
    "rnn_size = 300\n",
    "enc_dropout = 0.0\n",
    "\n",
    "# specially for transformer\n",
    "# num_heads = 2048\n",
    "# ff_size = 2048\n",
    "\n",
    "num_epoch = 100\n",
    "batch_size = 32\n",
    "learning_rate_ae = 1\n",
    "learning_rate_d = 0.1\n",
    "grad_clip = 1\n",
    "disc_layer = '{}-400-200'.format(num_layers*rnn_size)\n",
    "\n",
    "valid_every = 5\n",
    "checkpoint_every = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Path to textfile, Name, Use4Vocab)\n",
    "datafiles = [\n",
    "    ('data/yelp/pos_train.txt', \"train0\", True),\n",
    "    ('data/yelp/neg_train.txt', \"train1\", True),\n",
    "    ('data/yelp/pos_valid.txt', \"valid0\", False),\n",
    "    ('data/yelp/neg_valid.txt', \"valid1\", False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab 9599; Pruned to 9603\n",
      "Number of sentences dropped from data/yelp/pos_train.txt: 0 out of 267314 total\n",
      "Number of sentences dropped from data/yelp/neg_train.txt: 0 out of 176787 total\n",
      "Number of sentences dropped from data/yelp/pos_valid.txt: 0 out of 38205 total\n",
      "Number of sentences dropped from data/yelp/neg_valid.txt: 0 out of 25278 total\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(datafiles,\n",
    "                maxlen=max_length,\n",
    "                vocab_size=vocab_size,\n",
    "                lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 9603\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(corpus.dictionary.word2idx)\n",
    "print(\"Vocabulary Size: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5e5d707086ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain0_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain1_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalid0_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalid1_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/iceic/utils.py\u001b[0m in \u001b[0;36mbatchify\u001b[0;34m(data, bsz, shuffle, gpu)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# source has no end symbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;31m# target has no start symbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/iceic/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# source has no end symbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;31m# target has no start symbol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train0_data = batchify(corpus.data['train0'], batch_size, shuffle=True)\n",
    "train1_data = batchify(corpus.data['train1'], batch_size, shuffle=True)\n",
    "\n",
    "valid0_data = batchify(corpus.data['valid0'], batch_size, shuffle=False)\n",
    "valid1_data = batchify(corpus.data['valid1'], batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, encoder, decoder0, decoder1):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder0 = decoder0\n",
    "        self.decoder1 = decoder1\n",
    "        self.generator = None\n",
    "        \n",
    "        self.choose_decoder = lambda dec_idx: decoder0 if dec_idx == 0 else decoder1\n",
    "        \n",
    "    def forward(self, indices, lengths, dec_idx, only_enc=False):\n",
    "        # Encode\n",
    "        enc_final, memory_bank = self.encoder(indices, lengths)\n",
    "        if only_enc:\n",
    "            return torch.cat([enc_final[0:enc_final.size(0):2], enc_final[1:enc_final.size(0):2]], 2).squeeze(0)\n",
    "        \n",
    "        # Decode\n",
    "        assert dec_idx == 0 or dec_idx == 1\n",
    "        \n",
    "#        enc_state = self.choose_decoder(dec_idx).init_decoder_state(indices, memory_bank, enc_final)\n",
    "        enc_state = enc_final\n",
    "        decoder_outputs, dec_state, attns = self.choose_decoder(dec_idx)(indices, memory_bank, enc_state, memory_lengths=lengths)\n",
    "        decoded = self.generator(decoder_outputs)\n",
    "        \n",
    "        return decoded #decoder_outputs, attns, dec_state\n",
    "    \n",
    "    def generate(self, indices, lengths, dec_idx):\n",
    "        assert dec_idx == 0 or dec_idx == 1\n",
    "        batch_size = indices.size(1)\n",
    "        \n",
    "        enc_final, memory_bank = self.encoder(indices, lengths)\n",
    "        \n",
    "        token = torch.full((1, batch_size), SOS_IDX, dtype=torch.long, device=device).unsqueeze(2)\n",
    "        dec_state = self.choose_decoder(dec_idx).init_decoder_state(indices, memory_bank, enc_final)\n",
    "        \n",
    "        # unroll\n",
    "        all_indices = []\n",
    "        for i in range(max_length):\n",
    "            decoder_outputs, dec_state, attns = self.choose_decoder(dec_idx)(token, memory_bank, dec_state, memory_lengths=lengths)\n",
    "            output = self.generator(decoder_outputs)\n",
    "            topv, topi = output.topk(1, dim=2)\n",
    "            \n",
    "            all_indices.append(topi.squeeze(0).cpu())\n",
    "\n",
    "        all_indices = torch.cat(all_indices, dim=1)\n",
    "\n",
    "        return all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ninput, noutput, layers, activation=nn.ReLU(), device=torch.device(\"cpu\")):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ninput = ninput\n",
    "        self.noutput = noutput\n",
    "\n",
    "        layer_sizes = [ninput] + [int(x) for x in layers.split('-')]\n",
    "        self.layers = []\n",
    "\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            layer = nn.Linear(layer_sizes[i], layer_sizes[i+1]).to(device)\n",
    "            self.layers.append(layer)\n",
    "            self.add_module(\"layer\"+str(i+1), layer)\n",
    "\n",
    "            # No batch normalization in first layer\n",
    "            if i != 0:\n",
    "                bn = nn.BatchNorm1d(layer_sizes[i+1]).to(device)\n",
    "                self.layers.append(bn)\n",
    "                self.add_module(\"bn\"+str(i+1), bn)\n",
    "\n",
    "            self.layers.append(activation)\n",
    "            self.add_module(\"activation\"+str(i+1), activation)\n",
    "\n",
    "        layer = nn.Linear(layer_sizes[-1], noutput).to(device)\n",
    "        self.layers.append(layer)\n",
    "        self.add_module(\"layer\"+str(len(self.layers)), layer)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_std = 0.02\n",
    "        for layer in self.layers:\n",
    "            try:\n",
    "                layer.weight.data.normal_(0, init_std)\n",
    "                layer.bias.data.fill_(0)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_type='GRU', device=torch.device(\"cpu\")):\n",
    "    assert model_type == 'GRU' or model_type == 'TRANS'\n",
    "    \n",
    "    # Build encoder.\n",
    "    src_embeddings = Embeddings(\n",
    "        word_vec_size=embed_dim,\n",
    "        word_vocab_size=vocab_size,\n",
    "        word_padding_idx=0,\n",
    "        position_encoding=(model_type == 'TRANS')\n",
    "    )\n",
    "    print('build_encoder embedding')\n",
    "    if model_type == 'GRU':\n",
    "        encoder = RNNEncoder(\n",
    "            model_type, bidirectional=bidirectional, num_layers=num_layers,\n",
    "            hidden_size=rnn_size, dropout=enc_dropout, embeddings=src_embeddings\n",
    "        )\n",
    "    elif model_type == 'TRANS':\n",
    "        encoder = TransformerEncoder(\n",
    "            num_layers=num_layers, d_model=rnn_size, heads=num_heads,\n",
    "            d_ff=ff_size, dropout=enc_dropout, embeddings=src_embeddings\n",
    "        )\n",
    "        \n",
    "\n",
    "    print('build_encoder')\n",
    "    \n",
    "    # Build decoders.\n",
    "    tgt_embeddings0 = Embeddings(\n",
    "        word_vec_size=embed_dim,\n",
    "        word_vocab_size=vocab_size,\n",
    "        word_padding_idx=0,\n",
    "        position_encoding=(model_type == 'TRANS')\n",
    "    )\n",
    "    tgt_embeddings1 = Embeddings(\n",
    "        word_vec_size=embed_dim,\n",
    "        word_vocab_size=vocab_size,\n",
    "        word_padding_idx=0,\n",
    "        position_encoding=(model_type == 'TRANS')\n",
    "    )\n",
    "\n",
    "    print('build_decoder embedding')\n",
    "    \n",
    "    if model_type == 'GRU':\n",
    "        decoder0 = StdRNNDecoder(\n",
    "            rnn_type=model_type,\n",
    "            bidirectional_encoder=bidirectional,\n",
    "            num_layers=num_layers,\n",
    "            hidden_size=rnn_size,\n",
    "            embeddings=tgt_embeddings0\n",
    "        )\n",
    "        decoder1 = StdRNNDecoder(\n",
    "            rnn_type=model_type,\n",
    "            bidirectional_encoder=bidirectional,\n",
    "            num_layers=num_layers,\n",
    "            hidden_size=rnn_size,\n",
    "            embeddings=tgt_embeddings1\n",
    "        )\n",
    "    elif model_type == 'TRANS':\n",
    "        decoder0 = TransformerDecoder(\n",
    "            num_layers=num_layers,\n",
    "            d_model=rnn_size,\n",
    "            heads=num_heads,\n",
    "            d_ff=ff_size,\n",
    "            attn_type=None,\n",
    "            copy_attn=False,\n",
    "            self_attn_type=\"scaled-dot\",\n",
    "            dropout=0.0,\n",
    "            embeddings=tgt_embeddings0\n",
    "        )\n",
    "        decoder1 = TransformerDecoder(\n",
    "            num_layers=num_layers,\n",
    "            d_model=rnn_size,\n",
    "            heads=num_heads,\n",
    "            d_ff=ff_size,\n",
    "            attn_type=None,\n",
    "            copy_attn=False,\n",
    "            self_attn_type=\"scaled-dot\",\n",
    "            dropout=0.0,\n",
    "            embeddings=tgt_embeddings1\n",
    "        )\n",
    "\n",
    "    print('build_decoder')\n",
    "    \n",
    "    # Build Net(= encoder + decoder0 + decoder1).\n",
    "    model = Net(encoder, decoder0, decoder1)\n",
    "        \n",
    "    generator = nn.Sequential(\n",
    "        nn.Linear(rnn_size, vocab_size),\n",
    "        nn.LogSoftmax(dim=-1))\n",
    "    \n",
    "    if model_type == 'TRANS':\n",
    "        for p in model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "        for p in generator.parameters():\n",
    "            if p.dim() > 1:\n",
    "                xavier_uniform_(p)\n",
    "\n",
    "    '''\n",
    "    if hasattr(model.encoder, 'embeddings'):\n",
    "        model.encoder.embeddings.load_pretrained_vectors(\n",
    "            model_opt.pre_word_vecs_enc, model_opt.fix_word_vecs_enc)\n",
    "    if hasattr(model.decoder1, 'embeddings'):\n",
    "        model.decoder1.embeddings.load_pretrained_vectors(\n",
    "            model_opt.pre_word_vecs_dec, model_opt.fix_word_vecs_dec)\n",
    "    if hasattr(model.decoder2, 'embeddings'):\n",
    "        model.decoder2.embeddings.load_pretrained_vectors(\n",
    "            model_opt.pre_word_vecs_dec, model_opt.fix_word_vecs_dec)\n",
    "    '''\n",
    "    \n",
    "    # Add generator to model (this registers it as parameter of model).\n",
    "    model.generator = generator\n",
    "    model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "def initHIdden(cur_batch_size):\n",
    "    return torch.zeros(1 * 2, cur_batch_size, rnn_size, device=device)\n",
    "    \n",
    "def build_my_model(device=torch.device(\"cpu\")):    \n",
    "    class EncoderRNN(nn.Module):\n",
    "        def __init__(self, input_size, emb_size, hidden_size, n_layers, bidirectional=False):\n",
    "            super(EncoderRNN, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.emb_size = emb_size\n",
    "            self.hidden_size = hidden_size\n",
    "            self.n_layers = n_layers\n",
    "            self.bidirectional = bidirectional\n",
    "\n",
    "            self.embedding = nn.Embedding(input_size, emb_size, padding_idx=0)\n",
    "            self.rnn = nn.GRU(emb_size, hidden_size, bidirectional=self.bidirectional)\n",
    "\n",
    "        def forward(self, input_seqs, input_lens):\n",
    "            \"\"\"\n",
    "            Inputs is batch of sentences: BATCH_SIZE x MAX_LENGTH+1\n",
    "            \"\"\"\n",
    "            embedded = self.embedding(input_seqs)\n",
    "            packed = pack_padded_sequence(embedded, input_lens)\n",
    "            outputs, hidden = self.rnn(packed, initHIdden(input_seqs.size(1))) # default zero hidden\n",
    "            outputs, output_lengths = pad_packed_sequence(outputs)\n",
    "            return hidden, outputs\n",
    "\n",
    "    class DecoderRNN(nn.Module):\n",
    "        def __init__(self, hidden_size, emb_size, output_size,\n",
    "                     n_layers, dropout_p, bidirection, gpu_id=-1):\n",
    "            super(DecoderRNN, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.emb_size = emb_size\n",
    "            self.output_size = output_size\n",
    "            self.n_layers = n_layers\n",
    "            \n",
    "            self.dropout_p = dropout_p\n",
    "            self.gpu_id = gpu_id\n",
    "            self.bi_encoder = bidirection\n",
    "    \n",
    "            self.embedding = nn.Embedding(output_size, emb_size, padding_idx=0)\n",
    "            self.dropout = nn.Dropout(self.dropout_p)\n",
    "            self.rnn = nn.GRU(emb_size, hidden_size)\n",
    "            \n",
    "        def forward_step(self, input_var, hidden, encoder_outputs):\n",
    "            batch_size = input_var.size(0)\n",
    "            output_size = input_var.size(1)\n",
    "            \n",
    "            embedded = self.embedding(input_var)\n",
    "            embedded = self.dropout(embedded)\n",
    "    \n",
    "            output, hidden = self.rnn(embedded, hidden)\n",
    "            return output, hidden, None\n",
    "        \n",
    "        def forward(self, decoder_input, encoder_outputs, encoder_hidden, memory_lengths=None):\n",
    "            decoder_hidden = self._cat_directions(encoder_hidden) if self.bi_encoder else encoder_hidden\n",
    "            \n",
    "            decoder_output, decoder_hidden, _ = self.forward_step(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            return decoder_output.transpose(1, 0), decoder_hidden, None\n",
    "        \n",
    "        def _cat_directions(self, h):\n",
    "            \"\"\" If the encoder is bidirectional, do the following transformation.\n",
    "                (#directions * #layers, #batch, hidden_size) -> (#layers, #batch, #directions * hidden_size)\n",
    "            \"\"\"\n",
    "            h = torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\n",
    "            return h\n",
    "        \n",
    "    \n",
    "    # Build encoder.\n",
    "    print('build_encoder')\n",
    "    encoder = EncoderRNN(vocab_size, embed_dim, rnn_size, num_layers, bidirectional=bidirectional)\n",
    "    \n",
    "    # Build decoders.\n",
    "    print('build_decoder')\n",
    "    decoder0 = DecoderRNN(rnn_size*2, embed_dim, vocab_size, num_layers, 0.0, bidirectional)\n",
    "    decoder1 = DecoderRNN(rnn_size*2, embed_dim, vocab_size, num_layers, 0.0, bidirectional)\n",
    "\n",
    "    \n",
    "    # Build Net(= encoder + decoder0 + decoder1).\n",
    "    model = Net(encoder, decoder0, decoder1)\n",
    "        \n",
    "    generator = nn.Sequential(\n",
    "        nn.Linear(rnn_size*2, vocab_size),\n",
    "        nn.LogSoftmax(dim=-1))\n",
    "    \n",
    "    # Add generator to model (this registers it as parameter of model).\n",
    "    model.generator = generator\n",
    "    model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_my_ae(dec_idx, batch, temp=1):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    source, target, lengths = batch\n",
    "    source = source.to(device)\n",
    "    target = target.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    \n",
    "    # output: batch x seq_len x ntokens\n",
    "    output = model(source, lengths, dec_idx)\n",
    "    \n",
    "    # output_size: batch_size, maxlen, self.ntokens\n",
    "    flattened_output = output.view(-1, vocab_size)\n",
    "    \n",
    "    recon_loss = F.nll_loss(flattened_output/temp, target, ignore_index=0, size_average=True)\n",
    "    recon_loss.backward()\n",
    "    \n",
    "    # `clip_grad_norm` to prevent exploding gradient in RNNs / LSTMs\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)  # max norm\n",
    "    optimizer_ae.step()\n",
    "    \n",
    "    model.eval()\n",
    "    return recon_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(dec_idx, batch, temp=1):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    source, target, lengths = batch\n",
    "    source = source.unsqueeze(2).to(device)\n",
    "    target = target.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    \n",
    "    # output: batch x seq_len x ntokens\n",
    "    output = model(source, lengths, dec_idx)\n",
    "    \n",
    "    # output_size: batch_size, maxlen, self.ntokens\n",
    "    flattened_output = output.view(-1, vocab_size)\n",
    "    \n",
    "    recon_loss = F.nll_loss(flattened_output/temp, target, ignore_index=0, size_average=True)\n",
    "    recon_loss.backward()\n",
    "    \n",
    "    # `clip_grad_norm` to prevent exploding gradient in RNNs / LSTMs\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)  # max norm\n",
    "    optimizer_ae.step()\n",
    "    \n",
    "    model.eval()\n",
    "    return recon_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae_masking(dec_idx, batch, temp=1):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    source, target, lengths = batch\n",
    "    source = source.unsqueeze(2).to(device)\n",
    "    target = target.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    \n",
    "    # Create sentence length mask over padding\n",
    "    mask = target.gt(0)\n",
    "    masked_target = target.masked_select(mask)\n",
    "    # examples x ntokens\n",
    "    output_mask = mask.unsqueeze(1).expand(mask.size(0), vocab_size)\n",
    "    \n",
    "    # output: batch x seq_len x ntokens\n",
    "    output = model(source, lengths, dec_idx)\n",
    "    \n",
    "    # output_size: batch_size, maxlen, self.ntokens\n",
    "    flattened_output = output.view(-1, vocab_size)\n",
    "    \n",
    "    masked_output = flattened_output.masked_select(output_mask).view(-1, vocab_size)\n",
    "    recon_loss = F.nll_loss(masked_output/temp, masked_target, size_average=True)\n",
    "    recon_loss.backward()\n",
    "    \n",
    "    # `clip_grad_norm` to prevent exploding gradient in RNNs / LSTMs\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)  # max norm\n",
    "    optimizer_ae.step()\n",
    "    \n",
    "    return recon_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_disc(dec_idx, batch):\n",
    "    disc.train()\n",
    "    disc.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = source.unsqueeze(2).to(device)\n",
    "    batch_size = source.size(1)\n",
    "    labels = torch.full([batch_size], dec_idx, device=device)\n",
    "\n",
    "    # Train\n",
    "    encoded = model(source, lengths, -1, only_enc=True).detach()\n",
    "    scores = disc(encoded)\n",
    "    \n",
    "    disc_loss = F.binary_cross_entropy(scores.squeeze(1), labels)\n",
    "\n",
    "    pred = scores.data.round().squeeze(1)\n",
    "    accuracy = pred.eq(labels.data).float().mean()\n",
    "    \n",
    "    if accuracy < 0.99:\n",
    "        disc_loss.backward()\n",
    "        optimizer_d.step()\n",
    "    \n",
    "    return disc_loss.item(), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adv(dec_idx, batch, temp=1):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    source, target, lengths = batch\n",
    "    source = source.unsqueeze(2).to(device)\n",
    "    flipped_class = 1-dec_idx\n",
    "    batch_size = source.size(1)\n",
    "    labels = torch.full([batch_size], flipped_class, device=device)\n",
    "\n",
    "    # Train\n",
    "    encoded = model(source, lengths, -1, only_enc=True)\n",
    "    scores = disc(encoded)\n",
    "    \n",
    "    adv_loss = F.binary_cross_entropy(scores.squeeze(1), labels)\n",
    "    adv_loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer_ae.step()\n",
    "\n",
    "    return adv_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_ae(dec_idx, data, it, temp=1):\n",
    "    model.eval()\n",
    "    \n",
    "    file_name = \"model_ep{}_{}to{}.txt\".format(it, 1-dec_idx, dec_idx)\n",
    "    \n",
    "    with open(os.path.join(\"test\", file_name), 'w', encoding='utf8') as fp:\n",
    "        for batch in data:\n",
    "            source, target, lengths = batch\n",
    "            source = source.unsqueeze(2).to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            result = model.generate(source, lengths, dec_idx)\n",
    "\n",
    "            origin = source.squeeze(2).cpu().transpose(1, 0).numpy()\n",
    "            transfered = result.numpy()\n",
    "\n",
    "            for org, trans in zip(origin, transfered):\n",
    "                words = [corpus.dictionary.idx2word[x] for x in org]\n",
    "                fp.write(truncate(words) + '\\n')\n",
    "    \n",
    "                words = [corpus.dictionary.idx2word[x] for x in trans]\n",
    "                fp.write(truncate(words) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_ae_test(dec_idx, data, it, temp=1):\n",
    "    model.eval()\n",
    "    \n",
    "    file_name = \"model_test_ep{}_{}to{}.txt\".format(it, 1-dec_idx, dec_idx)\n",
    "    \n",
    "    with open(os.path.join(\"test\", file_name), 'w', encoding='utf8') as fp:\n",
    "        for batch in data:\n",
    "            source, target, lengths = batch\n",
    "            source = source.unsqueeze(2).to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            result = model.generate(source, lengths, dec_idx)\n",
    "\n",
    "            origin = source.squeeze(2).cpu().transpose(1, 0).numpy()\n",
    "            transfered = result.numpy()\n",
    "\n",
    "            for org, trans in zip(origin, transfered):\n",
    "                words = [corpus.dictionary.idx2word[x] for x in org]\n",
    "                fp.write(truncate(words) + '\\n')\n",
    "    \n",
    "                words = [corpus.dictionary.idx2word[x] for x in trans]\n",
    "                fp.write(truncate(words) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no checkpoint, model_opt == opt\n",
    "#model = build_model(model_type='GRU', device=device)\n",
    "model = build_my_model(device=device)\n",
    "disc = Discriminator(ninput=rnn_size, noutput=1, layers=disc_layer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer_ae = optim.SGD(model.parameters(), lr=learning_rate_ae)\n",
    "optimizer_ae = optim.SGD(model.parameters(), lr=1)\n",
    "optimizer_d = optim.SGD(disc.parameters(), lr=learning_rate_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "# \n",
    "######################################\n",
    "\n",
    "total_loss_ae0 = 0\n",
    "\n",
    "for ep in tqdm(range(num_epoch)):\n",
    "    for batch0, batch1 in zip(train0_data, train1_data):\n",
    "        total_loss_ae0 += train_my_ae(0, batch0)\n",
    "        \n",
    "    print(\"[*] epoch : {}/{} / recon_loss : {:6.3f}\".format(\n",
    "        ep+1,\n",
    "        num_epoch,\n",
    "        total_loss_ae0\n",
    "    ))\n",
    "    \n",
    "    total_loss_ae0 = 0\n",
    "        \n",
    "    if (ep+1) % valid_every == 0:\n",
    "        print(\"[*] Validate the model...\")\n",
    "        valid_ae_test(0, valid0_data, ep+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### # \n",
    "######################################\n",
    "\n",
    "total_losses = [0, 0, 0]\n",
    "\n",
    "for ep in tqdm(range(num_epoch)):\n",
    "    for batch0, batch1 in zip(train0_data, train1_data):\n",
    "        total_losses[0] += train_ae(0, batch0)\n",
    "        total_losses[0] += train_ae(1, batch1)\n",
    "        \n",
    "        total_losses[0] += train_disc(0, batch0)\n",
    "        total_losses[0] += train_disc(1, batch1)\n",
    "       \n",
    "#        if train_acc0 > 0.75:\n",
    "#            train_loss_adv0 = train_adv(disc, 0, batch0)\n",
    "#        if train_acc1 > 0.75:\n",
    "#            train_loss_adv1 = train_adv(disc, 1, batch1)\n",
    "        \n",
    "    print(\"[*] epoch : {}/{} / recon_loss : {:6.3f}\".format(\n",
    "        ep+1,\n",
    "        num_epoch,\n",
    "        total_loss_ae0\n",
    "    ))\n",
    "    \n",
    "    total_loss_ae0 = 0\n",
    "        \n",
    "    if (ep+1) % valid_every == 0:\n",
    "        print(\"[*] Validate the model...\")\n",
    "        valid_ae_test(0, valid0_data, ep+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(100)):\n",
    "    train_loss_ae0 = train_ae(0, train0_data[0])\n",
    "#next(model.parameters())[1]\n",
    "train_loss_ae0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ae(0, train0_data[:1], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
